{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet_try.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ttiq4yoiHAM",
        "outputId": "dd3478a3-9eb2-49dd-82f3-ce1e5171c6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python3-pyqt5\n",
        "!apt-get install python3-pyside\n",
        "!pip install pyside2\n",
        "!pip install labelme"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcQZCIyBiHrK",
        "outputId": "803053fe-daa8-4dbc-f61d-5ea16f8993ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-pyqt5 is already the newest version (5.10.1+dfsg-1ubuntu2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-pyside is already the newest version (1.2.2+source1-3).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "The folder you are executing pip from can no longer be found.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "The folder you are executing pip from can no longer be found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive/MyDrive/tensorflow-unet-labelme/datasets/train_voc\n",
        "!/content/drive/MyDrive/tensorflow-unet-labelme/labelme2voc.py /content/drive/MyDrive/tensorflow-unet-labelme/datasets/train /content/drive/MyDrive/tensorflow-unet-labelme/datasets/train_voc --labels /content/drive/MyDrive/tensorflow-unet-labelme/datasets/labels.txt\n",
        "!python /content/drive/MyDrive/tensorflow-unet-labelme/voc_annotation.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4F-aUHdiHl7",
        "outputId": "e3913a4d-effc-4015-9a31-3d8a9857bdb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "sh: 0: getcwd() failed: No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/tensorflow-unet-labelme/labelme2voc.py\", line 11, in <module>\n",
            "    import imgviz\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/imgviz/__init__.py\", line 5, in <module>\n",
            "    from . import _io as io\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/imgviz/_io/__init__.py\", line 11, in <module>\n",
            "    from .pyplot import pyplot_imshow\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/imgviz/_io/pyplot.py\", line 3, in <module>\n",
            "    import matplotlib.pyplot\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\", line 1015, in <module>\n",
            "    rcParams = rc_params()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\", line 878, in rc_params\n",
            "    return rc_params_from_file(matplotlib_fname(), fail_on_error)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\", line 731, in matplotlib_fname\n",
            "    for fname in gen_candidates():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\", line 720, in gen_candidates\n",
            "    yield os.path.join(os.getcwd(), 'matplotlibrc')\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "Generate txt in ImageSets.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/tensorflow-unet-labelme/voc_annotation.py\", line 25, in <module>\n",
            "    os.makedirs(saveBasePath)\n",
            "  File \"/usr/lib/python3.7/os.py\", line 213, in makedirs\n",
            "    makedirs(head, exist_ok=exist_ok)\n",
            "  File \"/usr/lib/python3.7/os.py\", line 213, in makedirs\n",
            "    makedirs(head, exist_ok=exist_ok)\n",
            "  File \"/usr/lib/python3.7/os.py\", line 213, in makedirs\n",
            "    makedirs(head, exist_ok=exist_ok)\n",
            "  File \"/usr/lib/python3.7/os.py\", line 223, in makedirs\n",
            "    mkdir(name, mode)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'datasets'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import warnings"
      ],
      "metadata": {
        "id": "2excbELoiHjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 4 # class number + 1 (background)\n",
        "INPUT_SHAPE = [480, 640, 3] # (H, W, C)\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 100\n",
        "VAL_SUBSPLITS = 1"
      ],
      "metadata": {
        "id": "0M9NR2ZkiHhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "\n",
        "def cvtColor(image):\n",
        "    if len(np.shape(image)) == 3 and np.shape(image)[-2] == 3:\n",
        "        return image\n",
        "    else:\n",
        "        image = image.convert('RGB')\n",
        "        return image\n",
        "\n",
        "\n",
        "def normalize(image):\n",
        "    image = image / 127.5 - 1\n",
        "    return image\n",
        "\n",
        "\n",
        "def resize_image(image, size):\n",
        "    iw, ih = image.size\n",
        "    w, h = size\n",
        "\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw = int(iw * scale)\n",
        "    nh = int(ih * scale)\n",
        "\n",
        "    image = image.resize((nw, nh), Image.BICUBIC)\n",
        "    new_image = Image.new('RGB', size, (128, 128, 128))\n",
        "    new_image.paste(image, ((w - nw) // 2, (h - nh) // 2))\n",
        "\n",
        "    return new_image, nw, nh\n",
        "\n",
        "\n",
        "def resize_label(image, size):\n",
        "    iw, ih = image.size\n",
        "    w, h = size\n",
        "\n",
        "    scale = min(w / iw, h / ih)\n",
        "    nw = int(iw * scale)\n",
        "    nh = int(ih * scale)\n",
        "\n",
        "    image = image.resize((nw, nh), Image.NEAREST)\n",
        "    new_image = Image.new('L', size, (0))\n",
        "    new_image.paste(image, ((w - nw) // 2, (h - nh) // 2))\n",
        "\n",
        "    return new_image, nw, nh\n",
        "\n",
        "\n",
        "class UnetDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, annotation_lines, input_shape, batch_size, num_classes,\n",
        "                 train, dataset_path):\n",
        "        self.annotation_lines = annotation_lines\n",
        "        self.length = len(self.annotation_lines)\n",
        "        self.input_shape = input_shape\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.train = train\n",
        "        self.dataset_path = dataset_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.annotation_lines) / float(self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        images = []\n",
        "        targets = []\n",
        "        for i in range(index * self.batch_size, (index + 1) * self.batch_size):\n",
        "            i = i % self.length\n",
        "            name = self.annotation_lines[i].split()[0]\n",
        "            jpg = Image.open(\n",
        "                os.path.join(os.path.join(self.dataset_path, \"JPEGImages\"),\n",
        "                             name + \".jpg\"))\n",
        "            png = Image.open(\n",
        "                os.path.join(\n",
        "                    os.path.join(self.dataset_path, \"SegmentationClassPNG\"),\n",
        "                    name + \".png\"))\n",
        "\n",
        "            jpg, png = self.process_data(jpg,\n",
        "                                         png,\n",
        "                                         self.input_shape,\n",
        "                                         random=self.train)\n",
        "\n",
        "            images.append(jpg)\n",
        "            targets.append(png)\n",
        "\n",
        "        images = np.array(images)\n",
        "        targets = np.array(targets)\n",
        "        return images, targets\n",
        "\n",
        "    def rand(self, a=0, b=1):\n",
        "        return np.random.rand() * (b - a) + a\n",
        "\n",
        "    def process_data(self, image, label, input_shape, random=True):\n",
        "        image = cvtColor(image)\n",
        "        label = Image.fromarray(np.array(label))\n",
        "        h, w, _ = input_shape\n",
        "\n",
        "        # resize\n",
        "        image, _, _ = resize_image(image, (w, h))\n",
        "        label, _, _ = resize_label(label, (w, h))\n",
        "\n",
        "        if random:\n",
        "            # flip\n",
        "            flip = self.rand() < .5\n",
        "            if flip:\n",
        "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        # np\n",
        "        image = np.array(image, np.float32)\n",
        "        image = normalize(image)\n",
        "\n",
        "        label = np.array(label)\n",
        "        label[label >= self.num_classes] = self.num_classes\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "91eLvXbxiHcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/tensorflow-unet-labelme/datasets/train_voc'\n",
        "\n",
        "# read dataset txt files\n",
        "with open(os.path.join(dataset_path, \"ImageSets/Segmentation/train.txt\"),\n",
        "          \"r\",\n",
        "          encoding=\"utf8\") as f:\n",
        "    train_lines = f.readlines()\n",
        "\n",
        "with open(os.path.join(dataset_path, \"ImageSets/Segmentation/val.txt\"),\n",
        "          \"r\",\n",
        "          encoding=\"utf8\") as f:\n",
        "    val_lines = f.readlines()\n",
        "\n",
        "train_batches = UnetDataset(train_lines, INPUT_SHAPE, BATCH_SIZE, NUM_CLASSES,\n",
        "                            True, dataset_path)\n",
        "val_batches = UnetDataset(val_lines, INPUT_SHAPE, BATCH_SIZE, NUM_CLASSES,\n",
        "                          False, dataset_path)\n",
        "\n",
        "STEPS_PER_EPOCH = len(train_lines) // BATCH_SIZE\n",
        "VALIDATION_STEPS = len(val_lines) // BATCH_SIZE // VAL_SUBSPLITS"
      ],
      "metadata": {
        "id": "jb5-n6VZiHWW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "6616f10e-1030-428d-b9a8-c53108b30e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-207-ebcb422acfbb>\", line 6, in <module>\n",
            "    encoding=\"utf8\") as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/tensorflow-unet-labelme/datasets/train_voc/ImageSets/Segmentation/train.txt'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def display(display_list):\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i + 1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "images, masks = train_batches.__getitem__(0)\n",
        "sample_image, sample_mask = images[0], masks[0]\n",
        "sample_mask = sample_mask[..., tf.newaxis]\n",
        "display([sample_image, sample_mask])"
      ],
      "metadata": {
        "id": "SUY30s89iUSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/tutorials/generative/pix2pix\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(\n",
        "        tf.keras.layers.Conv2DTranspose(filters,\n",
        "                                        size,\n",
        "                                        strides=2,\n",
        "                                        padding='same',\n",
        "                                        kernel_initializer=initializer,\n",
        "                                        use_bias=False))\n",
        "\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "UBWGo3JbiUQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=INPUT_SHAPE,\n",
        "                                               include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',  # 64x64\n",
        "    'block_3_expand_relu',  # 32x32\n",
        "    'block_6_expand_relu',  # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',  # 4x4\n",
        "]\n",
        "base_model_outputs = [\n",
        "    base_model.get_layer(name).output for name in layer_names\n",
        "]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input,\n",
        "                            outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False\n",
        "\n",
        "up_stack = [\n",
        "    upsample(512, 3),  # 4x4 -> 8x8\n",
        "    upsample(256, 3),  # 8x8 -> 16x16\n",
        "    upsample(128, 3),  # 16x16 -> 32x32\n",
        "    upsample(64, 3),  # 32x32 -> 64x64\n",
        "]\n",
        "\n",
        "\n",
        "def unet_model(output_channels: int):\n",
        "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = down_stack(inputs)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        concat = tf.keras.layers.Concatenate()\n",
        "        x = concat([x, skip])\n",
        "\n",
        "    # This is the last layer of the model\n",
        "    last = tf.keras.layers.Conv2DTranspose(filters=output_channels,\n",
        "                                           kernel_size=3,\n",
        "                                           strides=2,\n",
        "                                           padding='same')  #64x64 -> 128x128\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "metadata": {
        "id": "RwDhcpWyiUNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet_model(output_channels=NUM_CLASSES)\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "HJq_jzT6iUK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(pred_mask):\n",
        "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "    pred_mask = pred_mask[..., tf.newaxis]\n",
        "    return pred_mask[0]\n",
        "\n",
        "\n",
        "def show_predictions(dataset=None, num=1):\n",
        "    if dataset:\n",
        "        for image, mask in dataset.take(num):\n",
        "            pred_mask = model.predict(image)\n",
        "            display([image[0], mask[0], create_mask(pred_mask)])\n",
        "    else:\n",
        "        display([\n",
        "            sample_image, sample_mask,\n",
        "            create_mask(model.predict(sample_image[tf.newaxis, ...]))\n",
        "        ])\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        clear_output(wait=True)\n",
        "        show_predictions()\n",
        "        print('\\nSample Prediction after epoch {}\\n'.format(epoch + 1))\n",
        "\n",
        "\n",
        "class ModelCheckpointCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self,\n",
        "                 filepath,\n",
        "                 monitor='val_loss',\n",
        "                 verbose=0,\n",
        "                 save_best_only=False,\n",
        "                 save_weights_only=False,\n",
        "                 mode='auto',\n",
        "                 period=1):\n",
        "        super(ModelCheckpointCallback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.save_weights_only = save_weights_only\n",
        "        self.period = period\n",
        "        self.epochs_since_last_save = 0\n",
        "\n",
        "        if mode not in ['auto', 'min', 'max']:\n",
        "            warnings.warn(\n",
        "                'ModelCheckpoint mode %s is unknown, '\n",
        "                'fallback to auto mode.' % (mode), RuntimeWarning)\n",
        "            mode = 'auto'\n",
        "\n",
        "        if mode == 'min':\n",
        "            self.monitor_op = np.less\n",
        "            self.best = np.Inf\n",
        "        elif mode == 'max':\n",
        "            self.monitor_op = np.greater\n",
        "            self.best = -np.Inf\n",
        "        else:\n",
        "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
        "                self.monitor_op = np.greater\n",
        "                self.best = -np.Inf\n",
        "            else:\n",
        "                self.monitor_op = np.less\n",
        "                self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epochs_since_last_save += 1\n",
        "        if self.epochs_since_last_save >= self.period:\n",
        "            self.epochs_since_last_save = 0\n",
        "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
        "            if self.save_best_only:\n",
        "                current = logs.get(self.monitor)\n",
        "                if current is None:\n",
        "                    warnings.warn(\n",
        "                        'Can save best model only with %s available, '\n",
        "                        'skipping.' % (self.monitor), RuntimeWarning)\n",
        "                else:\n",
        "                    if self.monitor_op(current, self.best):\n",
        "                        if self.verbose > 0:\n",
        "                            print(\n",
        "                                '\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
        "                                ' saving model to %s' %\n",
        "                                (epoch + 1, self.monitor, self.best, current,\n",
        "                                 filepath))\n",
        "                        self.best = current\n",
        "                        if self.save_weights_only:\n",
        "                            self.model.save_weights(filepath, overwrite=True)\n",
        "                        else:\n",
        "                            self.model.save(filepath, overwrite=True)\n",
        "                    else:\n",
        "                        if self.verbose > 0:\n",
        "                            print('\\nEpoch %05d: %s did not improve' %\n",
        "                                  (epoch + 1, self.monitor))\n",
        "            else:\n",
        "                if self.verbose > 0:\n",
        "                    print('\\nEpoch %05d: saving model to %s' %\n",
        "                          (epoch + 1, filepath))\n",
        "                if self.save_weights_only:\n",
        "                    self.model.save_weights(filepath, overwrite=True)\n",
        "                else:\n",
        "                    self.model.save(filepath, overwrite=True)"
      ],
      "metadata": {
        "id": "brFf6DTWiUF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displayCallback = DisplayCallback()\n",
        "\n",
        "if not os.path.exists('logs'):\n",
        "    os.makedirs('logs')\n",
        "checkpointCallback = ModelCheckpointCallback(\n",
        "    'logs/ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
        "    monitor='val_loss',\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    period=1)\n",
        "\n",
        "model_history = model.fit(train_batches,\n",
        "                          epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=val_batches,\n",
        "                          callbacks=[displayCallback, checkpointCallback])\n",
        "\n",
        "model.save('logs/the-last-model.h5', overwrite=True)"
      ],
      "metadata": {
        "id": "ZsNseDb3ic2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
        "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X2ao_Quwicz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'logs/the-last-model.h5'\n",
        "model.load_weights(model_path)\n",
        "print('{} model loaded.'.format(model_path))\n",
        "\n",
        "import copy\n",
        "\n",
        "colors = [(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128),\n",
        "          (128, 0, 128), (0, 128, 128), (128, 128, 128),\n",
        "          (64, 0, 0), (192, 0, 0), (64, 128, 0), (192, 128, 0), (64, 0, 128),\n",
        "          (192, 0, 128), (64, 128, 128), (192, 128, 128), (0, 64, 0),\n",
        "          (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128),\n",
        "          (128, 64, 12)]\n",
        "\n",
        "\n",
        "def detect_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = cvtColor(image)\n",
        "\n",
        "    old_img = copy.deepcopy(image)\n",
        "    ori_h = np.array(image).shape[0]\n",
        "    ori_w = np.array(image).shape[1]\n",
        "\n",
        "    image_data, nw, nh = resize_image(image, (INPUT_SHAPE[1], INPUT_SHAPE[0]))\n",
        "\n",
        "    image_data = normalize(np.array(image_data, np.float32))\n",
        "\n",
        "    image_data = np.expand_dims(image_data, 0)\n",
        "\n",
        "    pr = model.predict(image_data)[0]\n",
        "\n",
        "    pr = pr[int((INPUT_SHAPE[0] - nh) // 2) : int((INPUT_SHAPE[0] - nh) // 2 + nh), \\\n",
        "            int((INPUT_SHAPE[1] - nw) // 2) : int((INPUT_SHAPE[1] - nw) // 2 + nw)]\n",
        "\n",
        "    pr = cv2.resize(pr, (ori_w, ori_h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    pr = pr.argmax(axis=-1)\n",
        "\n",
        "    # seg_img = np.zeros((np.shape(pr)[0], np.shape(pr)[1], 3))\n",
        "    # for c in range(NUM_CLASSES):\n",
        "    #     seg_img[:, :, 0] += ((pr[:, :] == c ) * colors[c][0]).astype('uint8')\n",
        "    #     seg_img[:, :, 1] += ((pr[:, :] == c ) * colors[c][1]).astype('uint8')\n",
        "    #     seg_img[:, :, 2] += ((pr[:, :] == c ) * colors[c][2]).astype('uint8')\n",
        "    seg_img = np.reshape(\n",
        "        np.array(colors, np.uint8)[np.reshape(pr, [-1])], [ori_h, ori_w, -1])\n",
        "\n",
        "    image = Image.fromarray(seg_img)\n",
        "    image = Image.blend(old_img, image, 0.7)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "VSrMizOqicxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_path = 'datasets/test/1966.jpg'\n",
        "\n",
        "image = detect_image(test_image_path)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lE0o-esdicvG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}